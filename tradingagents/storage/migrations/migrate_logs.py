#!/usr/bin/env python3
"""
å°† logs ç›®å½•ä¸‹çš„ tradingagents_structured.log* æ–‡ä»¶è¿ç§»åˆ° MongoDB

è¯¥å·¥å…·ä¼šï¼š
1. éå† logs ç›®å½•ä¸‹æ‰€æœ‰ tradingagents_structured.log* æ–‡ä»¶ï¼ˆé»˜è®¤åŒ…æ‹¬è½®è½¬æ–‡ä»¶ï¼Œå¯ä½¿ç”¨ --no-rotated æ’é™¤ï¼‰
2. è¯»å–æ¯è¡Œçš„ JSON æ ¼å¼æ—¥å¿—
3. æ¸…ç† ANSI é¢œè‰²ä»£ç 
4. ä¿å­˜åˆ° MongoDB çš„ trading_agents_logs é›†åˆ
5. æ”¯æŒå»é‡ï¼ˆåŸºäº timestamp + logger + message çš„ç»„åˆï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python -m tradingagents.storage.migrations.migrate_logs

    ä¸åŒ…å«è½®è½¬æ–‡ä»¶ï¼Œåªå¤„ç†ä¸»æ—¥å¿—æ–‡ä»¶
    python -m tradingagents.storage.migrations.migrate_logs --no-rotated

æˆ–è€…ç›´æ¥è¿è¡Œï¼š
    python tradingagents/storage/migrations/migrate_logs.py
"""

import os
import json
import re
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

# æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ä½¿ç”¨ tradingagents.utils.logging_manager çš„ logger
# åŸå› ï¼šè¿ç§»å·¥å…·ä¼šå°†æ—¥å¿—å†™å…¥ MongoDBï¼Œè€Œä½¿ç”¨ logging_manager çš„ logger ä¼šè§¦å‘
# MongoDBLogHandlerï¼Œå¯¼è‡´è¿ç§»å·¥å…·è‡ªå·±çš„æ—¥å¿—ä¹Ÿè¢«å†™å…¥ MongoDBï¼Œå½¢æˆæ­»å¾ªç¯ï¼š
# è¿ç§»å·¥å…·è®°å½•æ—¥å¿— -> MongoDBLogHandler å†™å…¥ MongoDB -> è¿ç§»å·¥å…·è¯»å–å¹¶è¿ç§» -> å†æ¬¡è®°å½•æ—¥å¿— -> ...
# å› æ­¤ä½¿ç”¨ print ç›´æ¥è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé¿å…æ­»å¾ªç¯
# from tradingagents.utils.logging_manager import get_logger
# logger = get_logger('tools')

try:
    from pymongo import MongoClient
    from pymongo.errors import ConnectionFailure, ServerSelectionTimeoutError, BulkWriteError
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False
    BulkWriteError = None
    print("âŒ pymongo æœªå®‰è£…ï¼Œæ— æ³•è¿æ¥ MongoDB")


class LogsMigrator:
    """å°†æ—¥å¿—æ–‡ä»¶è¿ç§»åˆ° MongoDB"""
    
    def __init__(self, logs_dir: str = "logs", batch_size: int = 1000, include_rotated: bool = True):
        """
        åˆå§‹åŒ–è¿ç§»å™¨
        
        Args:
            logs_dir: logs ç›®å½•è·¯å¾„
            batch_size: æ‰¹é‡æ’å…¥çš„å¤§å°ï¼ˆé»˜è®¤1000ï¼‰
            include_rotated: æ˜¯å¦åŒ…å«è½®è½¬æ–‡ä»¶ï¼ˆ.log.1, .log.2 ç­‰ï¼‰ï¼Œé»˜è®¤ True
        """
        if not MONGODB_AVAILABLE:
            raise ImportError("pymongo is not installed. Please install it with: pip install pymongo")
        
        self.logs_dir = Path(logs_dir)
        self.batch_size = batch_size
        self.include_rotated = include_rotated
        self.client = None
        self.db = None
        self.collection = None
        self.connected = False
        
        # è¿æ¥ MongoDB
        self._connect()
    
    def _connect(self):
        """è¿æ¥åˆ° MongoDBï¼ˆä½¿ç”¨ç»Ÿä¸€çš„è¿æ¥ç®¡ç†ï¼‰"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„è¿æ¥ç®¡ç†
            from tradingagents.storage.manager import get_mongo_collection
            
            self.collection = get_mongo_collection("trading_agents_logs")
            if self.collection is None:
                print("âŒ ç»Ÿä¸€è¿æ¥ç®¡ç†ä¸å¯ç”¨ï¼Œæ— æ³•è¿æ¥MongoDB")
                self.connected = False
                raise ConnectionError("ç»Ÿä¸€è¿æ¥ç®¡ç†ä¸å¯ç”¨")
            
            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            self._create_indexes()
            
            self.connected = True
            print(f"âœ… MongoDBè¿æ¥æˆåŠŸï¼ˆä½¿ç”¨ç»Ÿä¸€è¿æ¥ç®¡ç†ï¼‰: trading_agents_logs")
            
        except Exception as e:
            print(f"âŒ MongoDBè¿æ¥å¤±è´¥: {e}")
            self.connected = False
            raise
    
    def _create_indexes(self):
        """åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½"""
        try:
            # åˆ›å»ºå¤åˆç´¢å¼•ç”¨äºå»é‡æŸ¥è¯¢ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
            self.collection.create_index([("timestamp", 1), ("logger", 1), ("message", 1)], background=True)
            # åˆ›å»ºæ—¶é—´ç´¢å¼•ç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢
            self.collection.create_index([("timestamp", 1)], background=True)
            # åˆ›å»º logger ç´¢å¼•ç”¨äºæŒ‰æ—¥å¿—å™¨æŸ¥è¯¢
            self.collection.create_index([("logger", 1)], background=True)
            print("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºç´¢å¼•å¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")
    
    def find_log_files(self) -> List[Path]:
        """
        æŸ¥æ‰¾æ‰€æœ‰ tradingagents_structured.log* æ–‡ä»¶
        
        Returns:
            æ‰€æœ‰åŒ¹é…çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        log_files = []
        
        if not self.logs_dir.exists():
            print(f"âŒ logs ç›®å½•ä¸å­˜åœ¨: {self.logs_dir}")
            return log_files
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        # tradingagents_structured.log, tradingagents_structured.log.1, tradingagents_structured.log.2, etc.
        pattern = "tradingagents_structured.log*"
        
        for log_file in self.logs_dir.glob(pattern):
            if log_file.is_file():
                # å¦‚æœä¸åŒ…å«è½®è½¬æ–‡ä»¶ï¼Œåˆ™è·³è¿‡è½®è½¬æ–‡ä»¶
                if not self.include_rotated:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è½®è½¬æ–‡ä»¶ï¼ˆåŒ…å« .log.æ•°å­— æ ¼å¼ï¼‰
                    if re.search(r'\.log\.\d+$', log_file.name):
                        continue
                log_files.append(log_file)
        
        # æŒ‰æ–‡ä»¶åæ’åºï¼ˆä¸»æ–‡ä»¶åœ¨å‰ï¼Œè½®è½¬æ–‡ä»¶æŒ‰æ•°å­—é¡ºåºï¼‰
        log_files.sort(key=lambda x: self._get_log_file_order(x))
        
        print(f"ğŸ“ æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶{'ï¼ˆä¸åŒ…å«è½®è½¬æ–‡ä»¶ï¼‰' if not self.include_rotated else ''}")
        for log_file in log_files:
            print(f"   - {log_file.name}")
        
        return log_files
    
    def _get_log_file_order(self, log_file: Path) -> tuple:
        """
        è·å–æ—¥å¿—æ–‡ä»¶çš„æ’åºé¡ºåº
        
        Args:
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ’åºå…ƒç»„ï¼š(ä¸»æ–‡ä»¶=0, è½®è½¬ç¼–å·)
        """
        name = log_file.name
        if name == "tradingagents_structured.log":
            return (0, 0)
        
        # æå–è½®è½¬ç¼–å·
        match = re.search(r'\.(\d+)$', name)
        if match:
            return (1, int(match.group(1)))
        
        return (2, 0)
    
    def clean_ansi_codes(self, text: str) -> str:
        """
        æ¸…ç† ANSI é¢œè‰²ä»£ç 
        
        Args:
            text: åŒ…å« ANSI ä»£ç çš„æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        # ANSI è½¬ä¹‰åºåˆ—çš„æ­£åˆ™è¡¨è¾¾å¼
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        return ansi_escape.sub('', text)
    
    def parse_log_line(self, line: str) -> Optional[Dict[str, Any]]:
        """
        è§£ææ—¥å¿—è¡Œï¼ˆJSONæ ¼å¼ï¼‰
        
        Args:
            line: æ—¥å¿—è¡Œå†…å®¹
            
        Returns:
            è§£æåçš„æ—¥å¿—å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å› None
        """
        line = line.strip()
        if not line:
            return None
        
        try:
            log_entry = json.loads(line)
            
            # æ¸…ç† ANSI é¢œè‰²ä»£ç ï¼ˆä¸»è¦åœ¨ level å­—æ®µä¸­ï¼‰
            if 'level' in log_entry and isinstance(log_entry['level'], str):
                log_entry['level'] = self.clean_ansi_codes(log_entry['level'])
            
            # ç¡®ä¿ timestamp å­—æ®µå­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
            if 'timestamp' in log_entry:
                try:
                    # å°è¯•è§£æ ISO æ ¼å¼çš„æ—¶é—´æˆ³
                    if isinstance(log_entry['timestamp'], str):
                        dt = datetime.fromisoformat(log_entry['timestamp'].replace('Z', '+00:00'))
                        log_entry['timestamp'] = dt
                except Exception as e:
                    # é™é»˜å¿½ç•¥æ—¶é—´æˆ³è§£æå¤±è´¥ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆé¿å…è¾“å‡ºè¿‡å¤šï¼‰
                    pass
            
            # æ·»åŠ è¿ç§»å…ƒæ•°æ®
            log_entry['migrated_at'] = datetime.now()
            
            return log_entry
            
        except json.JSONDecodeError as e:
            # é™é»˜å¿½ç•¥ JSON è§£æå¤±è´¥ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆé¿å…è¾“å‡ºè¿‡å¤šï¼‰
            return None
        except Exception as e:
            # é™é»˜å¿½ç•¥è§£æå¤±è´¥ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆé¿å…è¾“å‡ºè¿‡å¤šï¼‰
            return None
    
    def filter_duplicates(self, log_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡è¿‡æ»¤é‡å¤çš„æ—¥å¿—æ¡ç›®
        
        Args:
            log_entries: æ—¥å¿—æ¡ç›®åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„æ—¥å¿—æ¡ç›®åˆ—è¡¨ï¼ˆä¸åŒ…å«é‡å¤é¡¹ï¼‰
        """
        if not self.connected or not log_entries:
            return log_entries
        
        try:
            # æ„å»ºæ‰¹é‡æŸ¥è¯¢æ¡ä»¶ï¼ˆä½¿ç”¨ $or æŸ¥è¯¢ï¼‰
            queries = []
            for entry in log_entries:
                queries.append({
                    "timestamp": entry.get('timestamp'),
                    "logger": entry.get('logger'),
                    "message": entry.get('message')
                })
            
            if not queries:
                return log_entries
            
            # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„è®°å½•
            existing_records = set()
            # ä½¿ç”¨ $or æŸ¥è¯¢ï¼Œä½† MongoDB å¯¹ $or æŸ¥è¯¢æœ‰é™åˆ¶ï¼Œæ‰€ä»¥åˆ†æ‰¹æŸ¥è¯¢
            batch_query_size = 100  # æ¯æ¬¡æŸ¥è¯¢100æ¡
            for i in range(0, len(queries), batch_query_size):
                batch_queries = queries[i:i + batch_query_size]
                if batch_queries:
                    or_query = {"$or": batch_queries}
                    existing = self.collection.find(
                        or_query,
                        {"timestamp": 1, "logger": 1, "message": 1}
                    )
                    for record in existing:
                        # åˆ›å»ºå”¯ä¸€æ ‡è¯†
                        key = (
                            record.get('timestamp'),
                            record.get('logger'),
                            record.get('message')
                        )
                        existing_records.add(key)
            
            # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„è®°å½•
            filtered_entries = []
            for entry in log_entries:
                key = (
                    entry.get('timestamp'),
                    entry.get('logger'),
                    entry.get('message')
                )
                if key not in existing_records:
                    filtered_entries.append(entry)
            
            return filtered_entries
            
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡å»é‡å¤±è´¥ï¼Œå°†æ’å…¥æ‰€æœ‰è®°å½•: {e}")
            return log_entries
    
    def save_batch_to_mongodb(self, log_entries: List[Dict[str, Any]], skip_duplicates: bool = True) -> int:
        """
        æ‰¹é‡ä¿å­˜æ—¥å¿—æ¡ç›®åˆ° MongoDB
        
        Args:
            log_entries: æ—¥å¿—æ¡ç›®åˆ—è¡¨
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤æ¡ç›®
            
        Returns:
            æˆåŠŸæ’å…¥çš„è®°å½•æ•°
        """
        if not self.connected:
            print("âŒ MongoDB æœªè¿æ¥")
            return 0
        
        if not log_entries:
            return 0
        
        try:
            # å¦‚æœéœ€è¦å»é‡ï¼Œå…ˆè¿‡æ»¤é‡å¤é¡¹
            if skip_duplicates:
                original_count = len(log_entries)
                log_entries = self.filter_duplicates(log_entries)
                if len(log_entries) < original_count:
                    print(f"  å»é‡: {original_count} -> {len(log_entries)} æ¡")
            
            if not log_entries:
                return 0
            
            # æ‰¹é‡æ’å…¥ï¼ˆordered=False è¡¨ç¤ºå³ä½¿éƒ¨åˆ†å¤±è´¥ä¹Ÿç»§ç»­æ’å…¥ï¼‰
            result = self.collection.insert_many(log_entries, ordered=False)
            
            return len(result.inserted_ids)
            
        except Exception as e:
            # å¤„ç†éƒ¨åˆ†æˆåŠŸçš„æƒ…å†µï¼ˆordered=False æ—¶ï¼Œå³ä½¿æœ‰é”™è¯¯ä¹Ÿä¼šç»§ç»­æ’å…¥ï¼‰
            # BulkWriteError åŒ…å«éƒ¨åˆ†æˆåŠŸçš„ç»“æœ
            if BulkWriteError and isinstance(e, BulkWriteError):
                # è®¡ç®—æˆåŠŸæ’å…¥çš„æ•°é‡
                write_errors = e.details.get('writeErrors', [])
                successful_count = len(log_entries) - len(write_errors)
                if successful_count > 0:
                    print(f"âš ï¸ æ‰¹é‡æ’å…¥éƒ¨åˆ†å¤±è´¥: {successful_count} æ¡æˆåŠŸï¼Œ{len(write_errors)} æ¡å¤±è´¥")
                else:
                    print(f"âŒ æ‰¹é‡æ’å…¥å…¨éƒ¨å¤±è´¥: {e}")
                return successful_count
            
            # å…¶ä»–é”™è¯¯
            print(f"âŒ æ‰¹é‡ä¿å­˜åˆ° MongoDB å¤±è´¥: {e}")
            return 0
    
    def migrate_file(self, log_file: Path, skip_duplicates: bool = True, dry_run: bool = False) -> Dict[str, Any]:
        """
        è¿ç§»å•ä¸ªæ—¥å¿—æ–‡ä»¶ï¼ˆä½¿ç”¨æ‰¹é‡æ’å…¥ï¼‰
        
        Args:
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤æ¡ç›®
            dry_run: å¦‚æœä¸º Trueï¼Œåªæ‰«æä¸å®é™…ä¿å­˜
            
        Returns:
            è¿ç§»ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "file": str(log_file),
            "total_lines": 0,
            "parsed": 0,
            "saved": 0,
            "skipped": 0,
            "failed": 0,
            "errors": []
        }
        
        try:
            print(f"ğŸ“– å¼€å§‹å¤„ç†æ–‡ä»¶: {log_file.name} (æ‰¹é‡å¤§å°: {self.batch_size})")
            
            batch = []  # æ‰¹é‡æ’å…¥ç¼“å†²åŒº
            
            with open(log_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    stats["total_lines"] += 1
                    
                    # è§£ææ—¥å¿—è¡Œ
                    log_entry = self.parse_log_line(line)
                    
                    if log_entry is None:
                        stats["skipped"] += 1
                        continue
                    
                    stats["parsed"] += 1
                    
                    if dry_run:
                        stats["saved"] += 1
                        if line_num % 1000 == 0:
                            print(f"  [DRY RUN] å·²å¤„ç† {line_num} è¡Œ")
                    else:
                        # æ·»åŠ åˆ°æ‰¹é‡ç¼“å†²åŒº
                        batch.append(log_entry)
                        
                        # å½“è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼Œæ‰§è¡Œæ‰¹é‡æ’å…¥
                        if len(batch) >= self.batch_size:
                            saved_count = self.save_batch_to_mongodb(batch, skip_duplicates=skip_duplicates)
                            stats["saved"] += saved_count
                            
                            # è®¡ç®—è·³è¿‡çš„æ•°é‡ï¼ˆå¦‚æœå¯ç”¨å»é‡ï¼‰
                            if skip_duplicates:
                                skipped_in_batch = len(batch) - saved_count
                                stats["skipped"] += skipped_in_batch
                            
                            batch = []  # æ¸…ç©ºç¼“å†²åŒº
                            
                            if line_num % (self.batch_size * 10) == 0:
                                print(f"  å·²å¤„ç† {line_num} è¡Œï¼Œå·²ä¿å­˜ {stats['saved']} æ¡ï¼Œè·³è¿‡ {stats['skipped']} æ¡")
                
                # å¤„ç†å‰©ä½™çš„è®°å½•
                if not dry_run and batch:
                    saved_count = self.save_batch_to_mongodb(batch, skip_duplicates=skip_duplicates)
                    stats["saved"] += saved_count
                    
                    if skip_duplicates:
                        skipped_in_batch = len(batch) - saved_count
                        stats["skipped"] += skipped_in_batch
            
            print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {log_file.name}")
            print(f"   æ€»è¡Œæ•°: {stats['total_lines']}, è§£ææˆåŠŸ: {stats['parsed']}, "
                  f"ä¿å­˜: {stats['saved']}, è·³è¿‡: {stats['skipped']}, å¤±è´¥: {stats['failed']}")
            
        except Exception as e:
            stats["failed"] += 1
            error_msg = f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}"
            stats["errors"].append(error_msg)
            print(f"âŒ {error_msg}")
        
        return stats
    
    def migrate(self, skip_duplicates: bool = True, dry_run: bool = False) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¿ç§»
        
        Args:
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤æ¡ç›®
            dry_run: å¦‚æœä¸º Trueï¼Œåªæ‰«ææ–‡ä»¶ä¸å®é™…ä¿å­˜
            
        Returns:
            è¿ç§»ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.connected:
            print("âŒ MongoDB æœªè¿æ¥ï¼Œæ— æ³•æ‰§è¡Œè¿ç§»")
            return {}
        
        stats = {
            "total_files": 0,
            "total_lines": 0,
            "total_parsed": 0,
            "total_saved": 0,
            "total_skipped": 0,
            "total_failed": 0,
            "files": [],
            "errors": []
        }
        
        print(f"ğŸš€ å¼€å§‹è¿ç§» {'(dry run)' if dry_run else ''}")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        log_files = self.find_log_files()
        stats["total_files"] = len(log_files)
        
        if stats["total_files"] == 0:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶")
            return stats
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for log_file in log_files:
            try:
                file_stats = self.migrate_file(log_file, skip_duplicates=skip_duplicates, dry_run=dry_run)
                stats["files"].append(file_stats)
                
                # ç´¯è®¡ç»Ÿè®¡
                stats["total_lines"] += file_stats["total_lines"]
                stats["total_parsed"] += file_stats["parsed"]
                stats["total_saved"] += file_stats["saved"]
                stats["total_skipped"] += file_stats["skipped"]
                stats["total_failed"] += file_stats["failed"]
                
                if file_stats["errors"]:
                    stats["errors"].extend(file_stats["errors"])
                
            except Exception as e:
                stats["total_failed"] += 1
                error_msg = f"{log_file}: {str(e)}"
                stats["errors"].append(error_msg)
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {error_msg}")
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print("=" * 60)
        print("ğŸ“Š è¿ç§»ç»Ÿè®¡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"  æ€»è¡Œæ•°: {stats['total_lines']}")
        print(f"  è§£ææˆåŠŸ: {stats['total_parsed']}")
        print(f"  ä¿å­˜æˆåŠŸ: {stats['total_saved']}")
        print(f"  è·³è¿‡: {stats['total_skipped']}")
        print(f"  å¤±è´¥: {stats['total_failed']}")
        
        if stats['errors']:
            print(f"  é”™è¯¯è¯¦æƒ…: {len(stats['errors'])} ä¸ªé”™è¯¯")
            for error in stats['errors'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                print(f"    - {error}")
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å°†æ—¥å¿—æ–‡ä»¶è¿ç§»åˆ° MongoDB")
    parser.add_argument(
        "--logs-dir",
        type=str,
        default="logs",
        help="logs ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: logsï¼‰"
    )
    parser.add_argument(
        "--no-skip-duplicates",
        action="store_true",
        help="ä¸è·³è¿‡é‡å¤æ¡ç›®ï¼ˆé»˜è®¤ä¼šè·³è¿‡é‡å¤ï¼‰"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="åªæ‰«ææ–‡ä»¶ï¼Œä¸å®é™…ä¿å­˜åˆ° MongoDB"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="æ‰¹é‡æ’å…¥çš„å¤§å°ï¼ˆé»˜è®¤: 1000ï¼‰"
    )
    parser.add_argument(
        "--no-rotated",
        action="store_true",
        help="ä¸åŒ…å«è½®è½¬æ–‡ä»¶ï¼ˆ.log.1, .log.2 ç­‰ï¼‰ï¼Œåªå¤„ç†ä¸»æ—¥å¿—æ–‡ä»¶"
    )
    
    args = parser.parse_args()
    
    try:
        migrator = LogsMigrator(
            logs_dir=args.logs_dir,
            batch_size=args.batch_size,
            include_rotated=not args.no_rotated
        )
        stats = migrator.migrate(
            skip_duplicates=not args.no_skip_duplicates,
            dry_run=args.dry_run
        )
        
        if args.dry_run:
            print("â„¹ï¸ è¿™æ˜¯ dry run æ¨¡å¼ï¼Œæœªå®é™…ä¿å­˜æ•°æ®")
        
        return 0 if stats.get("total_failed", 0) == 0 else 1
        
    except Exception as e:
        print(f"âŒ è¿ç§»å¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit(main())

"""
# ä½¿ç”¨é»˜è®¤æ‰¹é‡å¤§å°ï¼ˆ1000ï¼‰ï¼ŒåŒ…å«æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ï¼ˆåŒ…æ‹¬è½®è½¬æ–‡ä»¶ï¼‰
python -m tradingagents.storage.migrations.migrate_logs

# è‡ªå®šä¹‰æ‰¹é‡å¤§å°ï¼ˆä¾‹å¦‚5000ï¼‰
python -m tradingagents.storage.migrations.migrate_logs --batch-size 5000

# ä¸åŒ…å«è½®è½¬æ–‡ä»¶ï¼Œåªå¤„ç†ä¸»æ—¥å¿—æ–‡ä»¶
python -m tradingagents.storage.migrations.migrate_logs --no-rotated

# å…¶ä»–å‚æ•°ä¿æŒä¸å˜
python -m tradingagents.storage.migrations.migrate_logs --logs-dir logs --dry-run
"""
