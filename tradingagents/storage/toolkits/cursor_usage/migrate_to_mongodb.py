#!/usr/bin/env python3
"""
å°† cursor_usage ç›®å½•ä¸‹çš„ CSV æ–‡ä»¶è¿ç§»åˆ° MongoDB

è¯¥å·¥å…·ä¼šï¼š
1. éå† cursor_usage ç›®å½•ä¸‹æ‰€æœ‰ {account_name}-usage-events-{date}.csv æ–‡ä»¶
2. ä»æ–‡ä»¶åä¸­æå– account_name å’Œ date
3. è¯»å– CSV æ–‡ä»¶å†…å®¹
4. ä¸ºæ¯æ¡è®°å½•æ·»åŠ  account_name å­—æ®µ
5. ä¿å­˜åˆ° MongoDB çš„ cursor_usages é›†åˆ
6. æ”¯æŒå»é‡ï¼ˆåŸºäº account_name + Date + Model + Kind çš„ç»„åˆï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python -m tradingagents.storage.toolkits.cursor_usage.migrate_to_mongodb

æˆ–è€…ç›´æ¥è¿è¡Œï¼š
    python tradingagents/storage/toolkits/cursor_usage/migrate_to_mongodb.py
"""

import re
import pandas as pd
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

try:
    from pymongo.errors import BulkWriteError
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False
    BulkWriteError = None
    print("âŒ pymongo æœªå®‰è£…ï¼Œæ— æ³•è¿æ¥ MongoDB")


class CursorUsageMigrator:
    """å°† Cursor Usage CSV æ–‡ä»¶è¿ç§»åˆ° MongoDB"""
    
    def __init__(self, data_dir: Optional[Path] = None, batch_size: int = 1000):
        """
        åˆå§‹åŒ–è¿ç§»å™¨
        
        Args:
            data_dir: CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰æ¨¡å—ç›®å½•
            batch_size: æ‰¹é‡æ’å…¥çš„å¤§å°ï¼ˆé»˜è®¤1000ï¼‰
        """
        if not MONGODB_AVAILABLE:
            raise ImportError("pymongo is not installed. Please install it with: pip install pymongo")
        
        if data_dir is None:
            data_dir = Path(__file__).parent
        
        self.data_dir = Path(data_dir)
        self.batch_size = batch_size
        
        # ä½¿ç”¨ CursorUsageManager
        from tradingagents.storage.mongodb.cursor_usage_manager import CursorUsageManager
        self.manager = CursorUsageManager()
        
        if not self.manager.is_connected():
            raise ConnectionError("æ— æ³•è¿æ¥åˆ° MongoDB")
        
        print(f"âœ… MongoDBè¿æ¥æˆåŠŸ: cursor_usage")
    
    def _parse_filename(self, filename: str) -> Optional[Dict[str, str]]:
        """
        è§£ææ–‡ä»¶åï¼Œæå– account_name å’Œ date
        
        Args:
            filename: æ–‡ä»¶åï¼Œæ ¼å¼ä¸º {account_name}-usage-events-{date}.csv
            
        Returns:
            åŒ…å« account_name å’Œ date çš„å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
        """
        pattern = re.compile(r'^(.+?)-usage-events-(\d{4}-\d{2}-\d{2})\.csv$')
        match = pattern.match(filename)
        
        if match:
            return {
                'account_name': match.group(1),
                'date': match.group(2)
            }
        return None
    
    def _find_csv_files(self) -> List[Path]:
        """
        æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ ¼å¼çš„ CSV æ–‡ä»¶
        
        Returns:
            CSV æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        csv_files = []
        pattern = re.compile(r'^.+?-usage-events-\d{4}-\d{2}-\d{2}\.csv$')
        
        if not self.data_dir.exists():
            print(f"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return csv_files
        
        for file_path in self.data_dir.glob('*-usage-events-*.csv'):
            if pattern.match(file_path.name):
                csv_files.append(file_path)
        
        # æŒ‰æ–‡ä»¶åæ’åº
        csv_files.sort(key=lambda x: x.name)
        
        print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶")
        return csv_files
    
    def _csv_to_documents(self, csv_file: Path, account_name: str) -> List[Dict[str, Any]]:
        """
        å°† CSV æ–‡ä»¶è½¬æ¢ä¸º MongoDB æ–‡æ¡£åˆ—è¡¨
        
        Args:
            csv_file: CSV æ–‡ä»¶è·¯å¾„
            account_name: è´¦æˆ·åç§°
            
        Returns:
            MongoDB æ–‡æ¡£åˆ—è¡¨
        """
        documents = []
        
        try:
            df = pd.read_csv(csv_file)
            
            # è½¬æ¢ Date åˆ—ä¸º datetime
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date'])
            
            # è½¬æ¢æ•°å€¼åˆ—
            numeric_columns = [
                'Input (w/ Cache Write)', 'Input (w/o Cache Write)', 
                'Cache Read', 'Output Tokens', 'Total Tokens', 'Cost'
            ]
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            
            # å°† DataFrame è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            for _, row in df.iterrows():
                doc = row.to_dict()
                
                # æ·»åŠ  account_name å­—æ®µ
                doc['account_name'] = account_name
                
                # å°† Date è½¬æ¢ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²ï¼ˆMongoDB å­˜å‚¨ï¼‰
                if 'Date' in doc and pd.notna(doc['Date']):
                    if isinstance(doc['Date'], pd.Timestamp):
                        doc['Date'] = doc['Date'].to_pydatetime()
                
                # ç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®
                for col in numeric_columns:
                    if col in doc:
                        doc[col] = float(doc[col]) if pd.notna(doc[col]) else 0.0
                
                documents.append(doc)
            
        except Exception as e:
            print(f"âŒ è¯»å– CSV æ–‡ä»¶å¤±è´¥ ({csv_file.name}): {e}")
            raise
        
        return documents
    
    
    def migrate_file(self, csv_file: Path, skip_duplicates: bool = True, dry_run: bool = False) -> Dict[str, Any]:
        """
        è¿ç§»å•ä¸ª CSV æ–‡ä»¶
        
        Args:
            csv_file: CSV æ–‡ä»¶è·¯å¾„
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤æ¡ç›®
            dry_run: å¦‚æœä¸º Trueï¼Œåªæ‰«æä¸å®é™…ä¿å­˜
            
        Returns:
            è¿ç§»ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "file": str(csv_file),
            "account_name": None,
            "date": None,
            "total_records": 0,
            "saved": 0,
            "skipped": 0,
            "failed": 0,
            "errors": []
        }
        
        try:
            # è§£ææ–‡ä»¶å
            file_info = self._parse_filename(csv_file.name)
            if not file_info:
                stats["errors"].append(f"æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®: {csv_file.name}")
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ï¼ˆæ ¼å¼ä¸æ­£ç¡®ï¼‰: {csv_file.name}")
                return stats
            
            stats["account_name"] = file_info['account_name']
            stats["date"] = file_info['date']
            
            print(f"ğŸ“– å¼€å§‹å¤„ç†æ–‡ä»¶: {csv_file.name} (è´¦æˆ·: {file_info['account_name']}, æ—¥æœŸ: {file_info['date']})")
            
            # è¯»å– CSV æ–‡ä»¶
            documents = self._csv_to_documents(csv_file, file_info['account_name'])
            stats["total_records"] = len(documents)
            
            if dry_run:
                stats["saved"] = len(documents)
                print(f"  [DRY RUN] å°†æ’å…¥ {len(documents)} æ¡è®°å½•")
            else:
                # æ‰¹é‡æ’å…¥
                batch = []
                for i, doc in enumerate(documents):
                    batch.append(doc)
                    
                    # å½“è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼Œæ‰§è¡Œæ‰¹é‡æ’å…¥
                    if len(batch) >= self.batch_size:
                        saved_count = self.manager.insert_many(batch, skip_duplicates=skip_duplicates)
                        stats["saved"] += saved_count
                        stats["skipped"] += len(batch) - saved_count
                        batch = []
                        
                        if (i + 1) % (self.batch_size * 10) == 0:
                            print(f"  å·²å¤„ç† {i + 1}/{len(documents)} æ¡è®°å½•ï¼Œå·²ä¿å­˜ {stats['saved']} æ¡")
                
                # å¤„ç†å‰©ä½™çš„è®°å½•
                if batch:
                    saved_count = self.manager.insert_many(batch, skip_duplicates=skip_duplicates)
                    stats["saved"] += saved_count
                    stats["skipped"] += len(batch) - saved_count
                
                print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: æ€»è®¡ {stats['total_records']} æ¡ï¼Œå·²ä¿å­˜ {stats['saved']} æ¡ï¼Œè·³è¿‡ {stats['skipped']} æ¡")
            
        except Exception as e:
            stats["failed"] = stats["total_records"]
            stats["errors"].append(str(e))
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ ({csv_file.name}): {e}")
        
        return stats
    
    def migrate_all(self, skip_duplicates: bool = True, dry_run: bool = False) -> Dict[str, Any]:
        """
        è¿ç§»æ‰€æœ‰ CSV æ–‡ä»¶
        
        Args:
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤æ¡ç›®
            dry_run: å¦‚æœä¸º Trueï¼Œåªæ‰«æä¸å®é™…ä¿å­˜
            
        Returns:
            æ€»ä½“è¿ç§»ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.manager.is_connected():
            print("âŒ MongoDB æœªè¿æ¥")
            return {}
        
        csv_files = self._find_csv_files()
        
        if not csv_files:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ ¼å¼çš„ CSV æ–‡ä»¶")
            return {}
        
        overall_stats = {
            "total_files": len(csv_files),
            "processed_files": 0,
            "total_records": 0,
            "total_saved": 0,
            "total_skipped": 0,
            "total_failed": 0,
            "file_stats": []
        }
        
        print(f"\nğŸš€ å¼€å§‹è¿ç§» {len(csv_files)} ä¸ªæ–‡ä»¶...")
        if dry_run:
            print("âš ï¸  DRY RUN æ¨¡å¼ï¼šåªæ‰«æï¼Œä¸å®é™…ä¿å­˜\n")
        
        for i, csv_file in enumerate(csv_files, 1):
            print(f"\n[{i}/{len(csv_files)}] ", end="")
            file_stats = self.migrate_file(csv_file, skip_duplicates=skip_duplicates, dry_run=dry_run)
            
            overall_stats["processed_files"] += 1
            overall_stats["total_records"] += file_stats["total_records"]
            overall_stats["total_saved"] += file_stats["saved"]
            overall_stats["total_skipped"] += file_stats["skipped"]
            overall_stats["total_failed"] += file_stats["failed"]
            overall_stats["file_stats"].append(file_stats)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¿ç§»å®Œæˆç»Ÿè®¡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {overall_stats['total_files']}")
        print(f"  å·²å¤„ç†: {overall_stats['processed_files']}")
        print(f"  æ€»è®°å½•æ•°: {overall_stats['total_records']}")
        print(f"  å·²ä¿å­˜: {overall_stats['total_saved']}")
        print(f"  å·²è·³è¿‡: {overall_stats['total_skipped']}")
        print(f"  å¤±è´¥: {overall_stats['total_failed']}")
        print(f"{'='*60}\n")
        
        return overall_stats


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å°† Cursor Usage CSV æ–‡ä»¶è¿ç§»åˆ° MongoDB')
    parser.add_argument('--data-dir', type=str, help='CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆé»˜è®¤ï¼šå½“å‰æ¨¡å—ç›®å½•ï¼‰')
    parser.add_argument('--batch-size', type=int, default=1000, help='æ‰¹é‡æ’å…¥å¤§å°ï¼ˆé»˜è®¤ï¼š1000ï¼‰')
    parser.add_argument('--no-skip-duplicates', action='store_true', help='ä¸è·³è¿‡é‡å¤è®°å½•')
    parser.add_argument('--dry-run', action='store_true', help='åªæ‰«æä¸å®é™…ä¿å­˜')
    
    args = parser.parse_args()
    
    try:
        data_dir = Path(args.data_dir) if args.data_dir else None
        migrator = CursorUsageMigrator(data_dir=data_dir, batch_size=args.batch_size)
        
        migrator.migrate_all(
            skip_duplicates=not args.no_skip_duplicates,
            dry_run=args.dry_run
        )
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

