#!/usr/bin/env python3
"""
AkShare News Provider Base Class

ä¸ºæ‰€æœ‰åŸºäº AkShare çš„æ–°é—»æä¾›å™¨æä¾›é€šç”¨åŠŸèƒ½
"""

from abc import abstractmethod
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
import pandas as pd

from .news_prov_base import NewsProvider
from tradingagents.news_engine.models import NewsItem, NewsSource
from tradingagents.utils.logging_manager import get_logger
from tradingagents.utils.time_utils import TaTimes

logger = get_logger("news_engine.akshare_base")


class AkShareNewsProviderBase(NewsProvider):
    """AkShare æ–°é—»æä¾›å™¨åŸºç±»"""

    def __init__(self, source: NewsSource, config_key: str):
        """
        åˆå§‹åŒ– AkShare æ–°é—»æä¾›å™¨

        Args:
            source: æ–°é—»æ¥æº
            config_key: é…ç½®é”®å,ç”¨äºæ£€æŸ¥æ˜¯å¦å¯ç”¨
        """
        super().__init__(source)
        self.config_key = config_key
        self._check_connection()

    def _check_connection(self):
        """æ£€æŸ¥æ•°æ®æºæ˜¯å¦å¯ç”¨ä»¥åŠä¾èµ–æ˜¯å¦å­˜åœ¨"""
        # æ£€æŸ¥é…ç½®å¼€å…³
        if not getattr(self.config, self.config_key, False):
            logger.debug(f"{self.source.value} æ•°æ®æºæœªå¯ç”¨ ({self.config_key}=false)")
            self.connected = False
            return

        try:
            import akshare  # noqa: F401

            self.connected = True
            logger.debug(f"âœ… {self.source.value} ä¾èµ–æ£€æŸ¥é€šè¿‡,æ•°æ®æºå¯ç”¨")
        except Exception as e:
            logger.error(f"âŒ {self.source.value} ä¾èµ–æ£€æŸ¥å¤±è´¥(akshare): {e}")
            self.connected = False

    def is_available(self) -> bool:
        """æ£€æŸ¥æ•°æ®æºæ˜¯å¦å¯ç”¨"""
        return self.connected

    @abstractmethod
    def _fetch_dataframe(self) -> pd.DataFrame:
        """
        è·å–åŸå§‹æ•°æ® DataFrame (å­ç±»å®ç°)

        Returns:
            åŒ…å«æ–°é—»æ•°æ®çš„ DataFrame
        """
        pass

    @abstractmethod
    def _get_column_mapping(self) -> Dict[str, str]:
        """
        è·å–åˆ—åæ˜ å°„ (å­ç±»å®ç°)

        Returns:
            åˆ—åæ˜ å°„å­—å…¸,æ ¼å¼:
            {
                'title': 'æ ‡é¢˜åˆ—å',
                'content': 'å†…å®¹åˆ—å',
                'date': 'æ—¥æœŸåˆ—å',
                'time': 'æ—¶é—´åˆ—å',  # å¯é€‰
                'url': 'URLåˆ—å',    # å¯é€‰
            }
        """
        pass

    def get_news(
        self,
        stock_code: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        max_news: int = 10,
    ) -> List[NewsItem]:
        """
        è·å–æ–°é—»æ•°æ®

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            max_news: æœ€å¤§æ–°é—»æ•°é‡

        Returns:
            æ–°é—»é¡¹ç›®åˆ—è¡¨
        """
        if not self.is_available():
            logger.debug(f"{self.source.value} æ•°æ®æºä¸å¯ç”¨,è·³è¿‡è·å–")
            return []

        # è®¡ç®—æ—¶é—´èŒƒå›´
        start_dt, end_dt = self._parse_date_range(start_date, end_date)

        logger.info(
            f"ğŸ“ {self.source.value} è·å– {stock_code} çš„æ–°é—», æ—¶é—´èŒƒå›´: "
            f"{start_dt.strftime('%Y-%m-%d %H:%M:%S')} ~ {end_dt.strftime('%Y-%m-%d %H:%M:%S')}"
        )

        try:
            # è·å–åŸå§‹æ•°æ®
            df = self._fetch_dataframe()

            if df is None or df.empty:
                logger.warning(f"{self.source.value} è¿”å›ç©ºæ•°æ®")
                return []

            logger.debug(f"è·å–åˆ° {len(df)} æ¡æ•°æ®")

            # è½¬æ¢ä¸º NewsItem åˆ—è¡¨
            news_items = self._dataframe_to_news_items(
                df, stock_code, start_dt, end_dt, max_news
            )

            # è®°å½•ç»“æœ
            if len(news_items) == 0:
                logger.debug(
                    f"ğŸ“ {self.source.value} æœªæ‰¾åˆ° {stock_code} çš„ç›¸å…³æ–°é—» "
                    f"(æ—¶é—´èŒƒå›´: {start_dt.strftime('%Y-%m-%d %H:%M:%S')} ~ {end_dt.strftime('%Y-%m-%d %H:%M:%S')}, "
                    f"æ€»æ•°æ®: {len(df)})"
                )
            else:
                logger.info(f"ğŸ“ {self.source.value} æˆåŠŸè·å– {len(news_items)} æ¡ç›¸å…³æ–°é—»")

            return news_items

        except Exception as e:
            logger.error(f"è·å– {self.source.value} æ•°æ®å¤±è´¥: {e}")
            return []

    def _parse_date_range(
        self, start_date: Optional[str], end_date: Optional[str]
    ) -> tuple:
        """
        è§£ææ—¥æœŸèŒƒå›´

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (æ”¯æŒ YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS)
            end_date: ç»“æŸæ—¥æœŸ (æ”¯æŒ YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS)

        Returns:
            (start_dt, end_dt) å…ƒç»„
        """
        try:
            if end_date:
                try:
                    end_dt = datetime.strptime(end_date, "%Y-%m-%d %H:%M:%S")
                except ValueError:
                    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                    end_dt = end_dt.replace(hour=23, minute=59, second=59)
            else:
                end_dt = datetime.now()

            if start_date:
                try:
                    start_dt = datetime.strptime(start_date, "%Y-%m-%d %H:%M:%S")
                except ValueError:
                    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            else:
                # é»˜è®¤å›æº¯ 3 å¤©
                start_dt = end_dt - timedelta(days=3)
        except Exception as e:
            logger.warning(
                f"è§£ææ—¥æœŸå¤±è´¥(start_date={start_date}, end_date={end_date}): {e}"
            )
            end_dt = datetime.now()
            start_dt = end_dt - timedelta(days=3)

        return start_dt, end_dt

    def _dataframe_to_news_items(
        self,
        df: pd.DataFrame,
        stock_code: str,
        start_dt: datetime,
        end_dt: datetime,
        max_news: int,
    ) -> List[NewsItem]:
        """
        å°† DataFrame è½¬æ¢ä¸º NewsItem åˆ—è¡¨

        Args:
            df: åŸå§‹æ•°æ® DataFrame
            stock_code: è‚¡ç¥¨ä»£ç 
            start_dt: å¼€å§‹æ—¶é—´
            end_dt: ç»“æŸæ—¶é—´
            max_news: æœ€å¤§æ–°é—»æ•°é‡

        Returns:
            NewsItem åˆ—è¡¨
        """
        column_mapping = self._get_column_mapping()
        all_items: List[NewsItem] = []

        for _, row in df.iterrows():
            try:
                # è§£ææ—¶é—´
                publish_time = self._parse_publish_time(row, column_mapping)

                # è¿‡æ»¤æ—¶é—´èŒƒå›´
                if publish_time < start_dt or publish_time > end_dt:
                    continue

                # æå–æ ‡é¢˜å’Œå†…å®¹
                title = self._extract_field(row, column_mapping, "title", "")
                content = self._extract_field(row, column_mapping, "content", "")
                url = self._extract_field(row, column_mapping, "url", "")

                # ç›¸å…³æ€§åˆ¤æ–­
                if stock_code:
                    if not self._is_related_to_stock(title, content, stock_code):
                        continue
                    urgency = self.assess_urgency(title, content)
                    relevance = self.calculate_relevance(title, stock_code)
                else:
                    urgency = 0
                    relevance = 0

                # åˆ›å»º NewsItem
                news_item = NewsItem(
                    title=title,
                    content=content,
                    source=self.source,
                    publish_time=publish_time,
                    url=url,
                    urgency=urgency,
                    relevance_score=relevance,
                    stock_code=stock_code,
                )
                all_items.append(news_item)

                if len(all_items) >= max_news:
                    break

            except Exception as e:
                logger.warning(f"è§£ææ•°æ®è¡Œå¤±è´¥: {e}")
                continue

        return all_items

    def _parse_publish_time(
        self, row: pd.Series, column_mapping: Dict[str, str]
    ) -> datetime:
        """
        è§£æå‘å¸ƒæ—¶é—´

        Args:
            row: DataFrame è¡Œ
            column_mapping: åˆ—åæ˜ å°„

        Returns:
            å‘å¸ƒæ—¶é—´
        """
        try:
            # å°è¯•ç»„åˆæ—¥æœŸå’Œæ—¶é—´åˆ—
            if "date" in column_mapping and "time" in column_mapping:
                date_col = column_mapping["date"]
                time_col = column_mapping["time"]

                if date_col and time_col and date_col in row and time_col in row:
                    date_str = str(row[date_col])
                    time_str = str(row[time_col])

                    if date_str and time_str:
                        full_time_str = f"{date_str} {time_str}"
                        try:
                            return datetime.strptime(full_time_str, "%Y-%m-%d %H:%M:%S")
                        except:
                            # å°è¯•å…¶ä»–æ ¼å¼
                            return datetime.strptime(date_str, "%Y-%m-%d")

            # å°è¯•å•ç‹¬çš„æ—¶é—´åˆ—
            if "datetime" in column_mapping:
                datetime_col = column_mapping["datetime"]
                if datetime_col and datetime_col in row:
                    datetime_str = str(row[datetime_col])
                    try:
                        return datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S")
                    except:
                        try:
                            return datetime.strptime(datetime_str, "%Y-%m-%d")
                        except:
                            pass

            # å°è¯•æ—¥æœŸåˆ—
            if "date" in column_mapping:
                date_col = column_mapping["date"]
                if date_col and date_col in row:
                    date_str = str(row[date_col])
                    try:
                        return datetime.strptime(date_str, "%Y-%m-%d")
                    except:
                        pass

        except Exception as e:
            logger.debug(f"è§£ææ—¶é—´å¤±è´¥: {e}")

        # é»˜è®¤è¿”å›å½“å‰æ—¶é—´
        return datetime.now()

    def _extract_field(
        self, row: pd.Series, column_mapping: Dict[str, str], field: str, default: str
    ) -> str:
        """
        ä»è¡Œä¸­æå–å­—æ®µ

        Args:
            row: DataFrame è¡Œ
            column_mapping: åˆ—åæ˜ å°„
            field: å­—æ®µå
            default: é»˜è®¤å€¼

        Returns:
            å­—æ®µå€¼
        """
        if field in column_mapping:
            col_name = column_mapping[field]
            if col_name and col_name in row:
                value = row[col_name]
                return str(value) if pd.notna(value) else default
        return default

    def _is_related_to_stock(self, title: str, content: str, stock_code: str) -> bool:
        """
        åˆ¤æ–­æ–°é—»æ˜¯å¦ä¸æŒ‡å®šè‚¡ç¥¨ç›¸å…³

        Args:
            title: æ ‡é¢˜
            content: å†…å®¹
            stock_code: è‚¡ç¥¨ä»£ç 

        Returns:
            æ˜¯å¦ç›¸å…³
        """
        text = (title + " " + content).lower()
        code_lower = stock_code.lower()

        if code_lower in text:
            return True

        # æ•°å­—ä»£ç åŒ¹é…(é€‚ç”¨äº A è‚¡/æ¸¯è‚¡)
        pure_code = "".join(filter(str.isdigit, stock_code))
        if pure_code and pure_code in text:
            return True

        return False
