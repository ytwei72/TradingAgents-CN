"""
åˆ†æé…ç½®æ„å»ºå™¨
è´Ÿè´£æ ¹æ®å‚æ•°æ„å»ºåˆ†æé…ç½®ï¼Œå¤ç”¨å·²æœ‰çš„é…ç½®ç®¡ç†ç±»
"""

import os
from pathlib import Path
from typing import Dict, Any, Optional
from tradingagents.default_config import DEFAULT_CONFIG

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('web')


class AnalysisConfigBuilder:
    """åˆ†æé…ç½®æ„å»ºå™¨"""
    
    def __init__(self, project_root: Optional[Path] = None):
        """åˆå§‹åŒ–é…ç½®æ„å»ºå™¨"""
        self.project_root = project_root or Path(__file__).parent.parent.parent
    
    def build_config(
        self,
        llm_provider: str,
        llm_model: str,
        research_depth: int,
        market_type: str = "ç¾è‚¡"
    ) -> Dict[str, Any]:
        """
        æ„å»ºåˆ†æé…ç½®
        
        Args:
            llm_provider: LLMæä¾›å•†
            llm_model: æ¨¡å‹åç§°
            research_depth: ç ”ç©¶æ·±åº¦ï¼ˆ1-5ï¼‰
            market_type: å¸‚åœºç±»å‹
            
        Returns:
            é…ç½®å­—å…¸
        """
        # ä»é»˜è®¤é…ç½®å¼€å§‹
        config = DEFAULT_CONFIG.copy()
        
        # è®¾ç½®åŸºç¡€LLMé…ç½®
        config["llm_provider"] = llm_provider
        config["deep_think_llm"] = llm_model
        config["quick_think_llm"] = llm_model
        
        # æ ¹æ®ç ”ç©¶æ·±åº¦è®¾ç½®è¾©è®ºè½®æ¬¡
        config.update(self._get_debate_config(research_depth))
        
        # è®¾ç½®é€šç”¨é…ç½®
        config["memory_enabled"] = True
        config["online_tools"] = True
        
        # æ ¹æ®LLMæä¾›å•†è®¾ç½®æ¨¡å‹å’Œbackend_url
        config.update(self._get_provider_config(llm_provider, llm_model, research_depth))
        
        # è®¾ç½®è·¯å¾„é…ç½®
        config.update(self._get_path_config())
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_directories(config)
        
        logger.info(f"ğŸ“‹ [é…ç½®æ„å»º] ç ”ç©¶æ·±åº¦: {research_depth}, æä¾›å•†: {llm_provider}")
        logger.info(f"ğŸ“‹ [é…ç½®æ„å»º] å¿«é€Ÿæ¨¡å‹: {config['quick_think_llm']}, æ·±åº¦æ¨¡å‹: {config['deep_think_llm']}")
        
        return config
    
    def _get_debate_config(self, research_depth: int) -> Dict[str, Any]:
        """æ ¹æ®ç ”ç©¶æ·±åº¦è·å–è¾©è®ºé…ç½®"""
        depth_configs = {
            1: {"max_debate_rounds": 1, "max_risk_discuss_rounds": 1},
            2: {"max_debate_rounds": 1, "max_risk_discuss_rounds": 1},
            3: {"max_debate_rounds": 1, "max_risk_discuss_rounds": 2},
            4: {"max_debate_rounds": 2, "max_risk_discuss_rounds": 2},
            5: {"max_debate_rounds": 3, "max_risk_discuss_rounds": 3},
        }
        return depth_configs.get(research_depth, depth_configs[3])
    
    def _get_provider_config(
        self,
        llm_provider: str,
        llm_model: str,
        research_depth: int
    ) -> Dict[str, Any]:
        """æ ¹æ®LLMæä¾›å•†è·å–é…ç½®"""
        provider_configs = {
            "dashscope": lambda: self._get_dashscope_config(research_depth),
            "deepseek": lambda: self._get_deepseek_config(),
            "qianfan": lambda: self._get_qianfan_config(research_depth),
            "google": lambda: self._get_google_config(research_depth),
            "openai": lambda: self._get_openai_config(llm_model),
            "openrouter": lambda: self._get_openrouter_config(llm_model),
            "siliconflow": lambda: self._get_siliconflow_config(llm_model),
            "custom_openai": lambda: self._get_custom_openai_config(llm_model),
        }
        
        config_func = provider_configs.get(llm_provider.lower())
        if config_func:
            return config_func()
        
        # é»˜è®¤é…ç½®
        return {
            "backend_url": "https://api.openai.com/v1",
            "quick_think_llm": llm_model,
            "deep_think_llm": llm_model,
        }
    
    def _get_dashscope_config(self, research_depth: int) -> Dict[str, Any]:
        """è·å–DashScopeé…ç½®"""
        model_map = {
            1: ("qwen-turbo", "qwen-plus"),
            2: ("qwen-plus", "qwen-plus"),
            3: ("qwen-plus", "qwen3-max"),
            4: ("qwen-plus", "qwen3-max"),
            5: ("qwen3-max", "qwen3-max"),
        }
        quick_model, deep_model = model_map.get(research_depth, ("qwen-plus", "qwen3-max"))
        
        return {
            "backend_url": "https://dashscope.aliyuncs.com/api/v1",
            "quick_think_llm": quick_model,
            "deep_think_llm": deep_model,
        }
    
    def _get_deepseek_config(self) -> Dict[str, Any]:
        """è·å–DeepSeeké…ç½®"""
        return {
            "backend_url": "https://api.deepseek.com",
            "quick_think_llm": "deepseek-chat",
            "deep_think_llm": "deepseek-chat",
        }
    
    def _get_qianfan_config(self, research_depth: int) -> Dict[str, Any]:
        """è·å–åƒå¸†ï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰é…ç½®"""
        if research_depth <= 2:
            model = "ernie-3.5-8k"
        elif research_depth <= 4:
            model = "ernie-4.0-turbo-8k"
        else:
            model = "ernie-4.0-turbo-8k"
        
        logger.info(f"ğŸ¤– [åƒå¸†] å¿«é€Ÿæ¨¡å‹: {model}, æ·±åº¦æ¨¡å‹: {model}")
        return {
            "backend_url": "https://aip.baidubce.com",
            "quick_think_llm": model,
            "deep_think_llm": model,
        }
    
    def _get_google_config(self, research_depth: int) -> Dict[str, Any]:
        """è·å–Google AIé…ç½®"""
        model_map = {
            1: ("gemini-2.5-flash-lite-preview-06-17", "gemini-2.0-flash"),
            2: ("gemini-2.0-flash", "gemini-1.5-pro"),
            3: ("gemini-1.5-pro", "gemini-2.5-flash"),
            4: ("gemini-2.5-flash", "gemini-2.5-pro"),
            5: ("gemini-2.5-pro", "gemini-2.5-pro"),
        }
        quick_model, deep_model = model_map.get(research_depth, ("gemini-1.5-pro", "gemini-2.5-flash"))
        
        logger.info(f"ğŸ¤– [Google AI] å¿«é€Ÿæ¨¡å‹: {quick_model}, æ·±åº¦æ¨¡å‹: {deep_model}")
        return {
            "backend_url": "https://api.openai.com/v1",
            "quick_think_llm": quick_model,
            "deep_think_llm": deep_model,
        }
    
    def _get_openai_config(self, llm_model: str) -> Dict[str, Any]:
        """è·å–OpenAIé…ç½®"""
        logger.info(f"ğŸ¤– [OpenAI] ä½¿ç”¨æ¨¡å‹: {llm_model}")
        return {
            "backend_url": "https://api.openai.com/v1",
            "quick_think_llm": llm_model,
            "deep_think_llm": llm_model,
        }
    
    def _get_openrouter_config(self, llm_model: str) -> Dict[str, Any]:
        """è·å–OpenRouteré…ç½®"""
        logger.info(f"ğŸŒ [OpenRouter] ä½¿ç”¨æ¨¡å‹: {llm_model}")
        return {
            "backend_url": "https://openrouter.ai/api/v1",
            "quick_think_llm": llm_model,
            "deep_think_llm": llm_model,
        }
    
    def _get_siliconflow_config(self, llm_model: str) -> Dict[str, Any]:
        """è·å–SiliconFlowé…ç½®"""
        logger.info(f"ğŸŒ [SiliconFlow] ä½¿ç”¨æ¨¡å‹: {llm_model}")
        return {
            "backend_url": "https://api.siliconflow.cn/v1",
            "quick_think_llm": llm_model,
            "deep_think_llm": llm_model,
        }
    
    def _get_custom_openai_config(self, llm_model: str) -> Dict[str, Any]:
        """è·å–è‡ªå®šä¹‰OpenAIé…ç½®"""
        # å°è¯•ä»streamlit session stateè·å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        try:
            import streamlit as st
            custom_base_url = st.session_state.get("custom_openai_base_url", "https://api.openai.com/v1")
        except:
            custom_base_url = os.getenv("CUSTOM_OPENAI_BASE_URL", "https://api.openai.com/v1")
        
        logger.info(f"ğŸ”§ [è‡ªå®šä¹‰OpenAI] ä½¿ç”¨æ¨¡å‹: {llm_model}, APIç«¯ç‚¹: {custom_base_url}")
        return {
            "backend_url": custom_base_url,
            "custom_openai_base_url": custom_base_url,
            "quick_think_llm": llm_model,
            "deep_think_llm": llm_model,
        }
    
    def _get_path_config(self) -> Dict[str, Any]:
        """è·å–è·¯å¾„é…ç½®"""
        config = {}
        
        # æ•°æ®ç›®å½•
        if not os.getenv("TRADINGAGENTS_DATA_DIR"):
            config["data_dir"] = str(self.project_root / "data")
        else:
            env_data_dir = os.getenv("TRADINGAGENTS_DATA_DIR")
            if os.path.isabs(env_data_dir):
                config["data_dir"] = env_data_dir
            else:
                config["data_dir"] = str(self.project_root / env_data_dir)
        
        # ç»“æœç›®å½•
        if not os.getenv("TRADINGAGENTS_RESULTS_DIR"):
            config["results_dir"] = str(self.project_root / "results")
        else:
            env_results_dir = os.getenv("TRADINGAGENTS_RESULTS_DIR")
            if os.path.isabs(env_results_dir):
                config["results_dir"] = env_results_dir
            else:
                config["results_dir"] = str(self.project_root / env_results_dir)
        
        # ç¼“å­˜ç›®å½•
        if not os.getenv("TRADINGAGENTS_CACHE_DIR"):
            config["data_cache_dir"] = str(self.project_root / "tradingagents" / "dataflows" / "data_cache")
        else:
            env_cache_dir = os.getenv("TRADINGAGENTS_CACHE_DIR")
            if os.path.isabs(env_cache_dir):
                config["data_cache_dir"] = env_cache_dir
            else:
                config["data_cache_dir"] = str(self.project_root / env_cache_dir)
        
        return config
    
    def _ensure_directories(self, config: Dict[str, Any]) -> None:
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            config.get("data_dir"),
            config.get("results_dir"),
            config.get("data_cache_dir"),
        ]
        
        for directory in directories:
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        logger.info(f"ğŸ“ [ç›®å½•é…ç½®] æ•°æ®ç›®å½•: {config.get('data_dir')}")
        logger.info(f"ğŸ“ [ç›®å½•é…ç½®] ç»“æœç›®å½•: {config.get('results_dir')}")
        logger.info(f"ğŸ“ [ç›®å½•é…ç½®] ç¼“å­˜ç›®å½•: {config.get('data_cache_dir')}")

